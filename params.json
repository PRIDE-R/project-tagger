{"name":"Project-tagger","tagline":"A model to automatically assign project tags","body":"# PRIDE Archive Project Tagger\r\nJose A. Dianes  \r\n6 April 2015  \r\n# Introduction and Goals  \r\n\r\nThe [PRIDE Archive](http://www.ebi.ac.uk/pride/archive) uses a system of tags \r\nto classify datasets. So far these tags are manually assigned by curators. So \r\nfar the PRIDE teach considers the following tags:  \r\n\r\n- Biological  \r\n- Biomedical  \r\n- Cardiovascular  \r\n- Chromosome-centric Human Proteome Project (C-HPP)  \r\n- Human Proteome Project  \r\n- Metaproteomics  \r\n- PRIME-XS Project  \r\n- Technical  \r\n\r\nIn the present analysis, by trying to build a model that predict tags, we want\r\nto find out what are behind those tags. Is it information such as a project\r\nspecies or tissue relevant to identify a project as biomedical? Is it that\r\ninformation actually hidden in the dataset textual description?  \r\n\r\n# Getting and cleaning the data  \r\n\r\nLet's start by getting the latest `prideR` version form `GitHub`, together with \r\nsome other packages we will use to build our model.  \r\n\r\n\r\n```r\r\nlibrary(devtools)\r\ninstall_github(\"PRIDE-R/prideR\")\r\nlibrary(prideR)\r\nrequire(ggplot2)\r\ntheme_set(theme_linedraw())\r\nrequire(tm)\r\nlibrary(caTools)\r\nlibrary(randomForest)\r\nlibrary(pander)\r\n```\r\n\r\nNow we get all the projects available in the archive.  \r\n\r\n\r\n```r\r\narchive.df <- as.data.frame(search.list.ProjectSummary(\"\",0,20000))\r\n```\r\n\r\nKepp only those with a project tag assigned.  \r\n\r\n\r\n```r\r\narchive.df <- subset(archive.df, project.tags!=\"Not available\")\r\n```\r\n\r\n# The tags distribution  \r\n\r\nNow we have 619 tagged projects in the dataset. Let's have a \r\nquick look at how these tags are distributed.  \r\n\r\n\r\n```r\r\nggplot(data=archive.df, aes(x=as.factor(archive.df$project.tags))) +\r\n    geom_bar() +\r\n    coord_flip( ) +\r\n    ylab(\"Number of Projects\") +\r\n    xlab(\"Project Tag\")\r\n```\r\n\r\n![](https://github.com/PRIDE-R/project-tagger/blob/master/README_files/figure-html/unnamed-chunk-4-1.png?raw=true) \r\n\r\nAs we see, our data project tag distribution is very skewed. This will make \r\ndifficult to build good models, specially now that we have very few projects.  \r\n\r\n# Using meta-data  \r\n\r\nLet's start by trying to predict projects tags based on meta-data only. That is,\r\nby using information such as the *species*, *tissues*, and so on.  \r\n\r\n## Preparing meta-data  \r\n\r\nRight now we have all the metadata associated to PRIDE datasets in text format.\r\nIn order to use it to build a model we better convert it to categorica data.\r\nHowever, most of them have too many differnet values for R machine libraries to\r\nmake use of them as predictors. For that reason we need to aggregate those low\r\nfrequency values into a unique `Other` factor level.  \r\n\r\n\r\n```r\r\nspecies.counts <- table(archive.df$species)\r\nlow.freq.species <- as.factor(names(which(species.counts<3)))\r\narchive.df[archive.df$species %in% low.freq.species,]$species <- \"Other\"\r\narchive.df$species <- as.factor(archive.df$species)\r\n\r\ntissues.counts <- table(archive.df$tissues)\r\nlow.freq.tissues <- as.factor(names(which(tissues.counts<3)))\r\narchive.df[archive.df$tissues %in% low.freq.tissues,]$tissues <- \"Other\"\r\narchive.df$tissues <- as.factor(archive.df$tissues)\r\n\r\nptm.counts <- table(archive.df$ptm.names)\r\nlow.freq.ptm <- as.factor(names(which(ptm.counts<3)))\r\narchive.df[archive.df$ptm.names %in% low.freq.ptm,]$ptm.names <- \"Other\"\r\narchive.df$ptm.names <- as.factor(archive.df$ptm.names)\r\n\r\ninstrument.counts <- table(archive.df$instrument.names)\r\nlow.freq.instrument <- as.factor(names(which(instrument.counts<3)))\r\narchive.df[archive.df$instrument.names %in% low.freq.instrument,]$instrument.names <- \"Other\"\r\narchive.df$instrument.names <- as.factor(archive.df$instrument.names)\r\n\r\narchive.df$project.tags <- as.factor(archive.df$project.tags)\r\narchive.df$submissionType <- as.factor(archive.df$submissionType)\r\n```\r\n\r\n## Exploring metadata  \r\n\r\nLet's explore a bit how each piece of metadata is distributed accross each \r\nproject tag. This might give us some insight into useful predictos if we\r\nfind any species, tissue, etc., more present in a particular tag than others.  \r\n\r\nFor that we will build a linear model and check the statistical importance of each\r\npredictor.  \r\n\r\n\r\n```r\r\nmeta_log_model <- glm(\r\n    project.tags ~ species + tissues + ptm.names + instrument.names + num.assays + submissionType, \r\n    data=archive.df, \r\n    family=binomial)\r\n```\r\n\r\n```\r\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\r\n```\r\n\r\n```r\r\nmeta_log_model_summary <- summary(meta_log_model)\r\nrelevant_meta <- names(which(meta_log_model_summary$coefficients[,4]<0.05))\r\n```\r\n\r\nAnd we see that the following predictors are statistically significan when \r\npredicting tags **assuming a linear relationship**:  \r\n\r\n\r\n|                  Predictor                  |\r\n|:-------------------------------------------:|\r\n|         speciesHomo sapiens (Human)         |\r\n|         speciesMus musculus (Mouse)         |\r\n|       speciesRattus norvegicus (Rat)        |\r\n| speciesSus scrofa domesticus (domestic pig) |\r\n|             submissionTypePRIDE             |\r\n\r\nWe cannot conclude much due to two factors. First, our relationship is surely\r\nnot linear. And second, our dataset is too small to model better relationships.\r\nBut at least it seems to be some relationship between *species*, and \r\n*submission type*, and the project tag.  \r\n\r\n## A meta-data only model  \r\n\r\nLet's try now a *Random Forest* model using just metadata predictors. We know \r\nour dependent variable is very skewed, so don't expect too much accuracy. We \r\ndecide for this type of model due to its success with non-linear problems.  \r\n\r\nFirst of all, prepare a train/test split so we can check accuracy. We also \r\nhave the problem of having too few data in general. Hopefully our models will\r\nget better while PRIDE datasets get more numerous.  \r\n\r\n\r\n```r\r\nset.seed(123)\r\nspl <- sample.split(archive.df, .85)\r\n\r\nevalTrain <- archive.df[spl==T,]\r\nevalTest <- archive.df[spl==F,]\r\n```\r\n\r\nAnd now train the model.  \r\n\r\n\r\n```r\r\nset.seed(123)\r\nrfModelMeta <- randomForest(\r\n    project.tags ~ species + tissues + ptm.names + instrument.names + submissionType, \r\n    data = evalTrain)\r\n# Calculate accuracy on the test set\r\nrfPredMeta <- predict(rfModelMeta, newdata=evalTest)\r\nt <- table(evalTest$project.tags, rfPredMeta)\r\nmeta_accuracy <- (t[1,1] + t[2,2] + t[3,3] + t[4,4] + t[5,5] + t[6,6] + t[7,7] + t[8,8]) / nrow(evalTest)\r\n```\r\n\r\nWe have a prediction accuracy of **0.7232143** on the test data. We can \r\nalso see how the model struggles to predict on classes with very few cases and\r\ndoes better with large classes.   \r\n\r\n\r\n|                         &nbsp;                          |  Biological  |  Biomedical  |  Cardiovascular  |  Chromosome-centric Human Proteome Project (C-HPP)  |  Human Proteome Project  |  Metaproteomics  |  PRIME-XS Project  |  Technical  |\r\n|:-------------------------------------------------------:|:------------:|:------------:|:----------------:|:---------------------------------------------------:|:------------------------:|:----------------:|:------------------:|:-----------:|\r\n|                     **Biological**                      |      38      |      8       |        0         |                          0                          |            0             |        1         |         0          |      1      |\r\n|                     **Biomedical**                      |      8       |      41      |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                   **Cardiovascular**                    |      0       |      1       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|  **Chromosome-centric Human Proteome Project (C-HPP)**  |      0       |      0       |        0         |                          1                          |            0             |        0         |         0          |      0      |\r\n|               **Human Proteome Project**                |      0       |      0       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                   **Metaproteomics**                    |      2       |      3       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                  **PRIME-XS Project**                   |      0       |      3       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                      **Technical**                      |      2       |      2       |        0         |                          0                          |            0             |        0         |         0          |      1      |\r\n\r\n# Using text fields  \r\n\r\nSurely PRIDE curators use textual description about datasets in order to assign\r\na tag to them. Let's try to incorporate that information into our models in order\r\nto predict a dataset tag.  \r\n\r\n## Preparing the corpus  \r\n\r\nWe have two textual fields, `project.title` and  `project.description`. Let's \r\nprepare a corpus with both of them. In both cases we will reduce them to \r\nlowercase, remove punctuation and stopwords, and apply stemming.   \r\n\r\n\r\n```r\r\nlibrary(tm)\r\nevalTrain$AllText <- do.call(paste, evalTrain[,c(\"project.title\",\"project.description\")])\r\nevalTest$AllText <- do.call(paste, evalTest[,c(\"project.title\",\"project.description\")])\r\n\r\ncorpusAll <- Corpus(VectorSource(c(evalTrain$AllText, evalTest$AllText)))\r\ncorpusAll <- tm_map(corpusAll, tolower)\r\ncorpusAll <- tm_map(corpusAll, PlainTextDocument)\r\ncorpusAll <- tm_map(corpusAll, removePunctuation)\r\ncorpusAll <- tm_map(corpusAll, removeWords, stopwords(\"english\"))\r\ncorpusAll <- tm_map(corpusAll, stripWhitespace)\r\ncorpusAll <- tm_map(corpusAll, stemDocument)\r\n```\r\n\r\nWe will also keep just those terms appearing in at least 3 percent of the projects.  \r\n\r\n\r\n```r\r\n# Generate term matrix\r\ndtmAll <- DocumentTermMatrix(corpusAll)\r\nsparseAll <- removeSparseTerms(dtmAll, 0.99)\r\nallWords <- data.frame(as.matrix(sparseAll))\r\n\r\ncolnames(allWords) <- make.names(colnames(allWords))\r\n```\r\n\r\n## Selecting significative terms  \r\n\r\nWe have ended up with **214 possible predictors**. But we can do \r\nbetter than this. We are going to train a *linear model* using them and our\r\ndependent variable as outcome. Then we will get those variables that are\r\nstatistically significative and incorporate them to the main dataset that\r\nwe will use with our final model.  \r\n\r\n\r\n```r\r\n# Find most significative terms\r\nallWordsTrain2 <- head(allWords, nrow(evalTrain))\r\nallWordsTrain2$Popular <- evalTrain$project.tag\r\nlogModelAllWords <- glm(Popular~., data=allWordsTrain2, family=binomial)\r\nall_three_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.001))\r\nall_two_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.01))\r\nall_one_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.05))\r\n\r\n# Leave just those terms that are different between popular and unpopular articles\r\nallWords <- subset(allWords, \r\n                   select=names(allWords) %in% all_one_star_terms)\r\n\r\n# Split again\r\nallWordsTrain <- head(allWords, nrow(evalTrain))\r\nallWordsTest <- tail(allWords, nrow(evalTest))\r\n\r\n# Add to dataframes\r\nevalTrain <- cbind(evalTrain, allWordsTrain)\r\nevalTest <- cbind(evalTest, allWordsTest)\r\n\r\n# Remove all text variable since we don't need it\r\nevalTrain$AllText <- NULL\r\nevalTest$AllText <- NULL\r\n```\r\n\r\nWe ended up with just **35 predictors** that will be \r\nincorporated into our model. These are the terms together with their mean \r\nfrequencies by project tag (remember they are **stemmed**):  \r\n\r\n\r\n```r\r\nallWords$project.tag <- archive.df$project.tag\r\npanderOptions('round', 2)\r\npanderOptions('keep.trailing.zeros', TRUE)\r\npander(t(aggregate(.~project.tag, data=allWords, mean)), style = \"rmarkdown\", split.table = Inf)\r\n```\r\n\r\n\r\n|                    |             |             |                |                                                   |                        |                |                  |             |\r\n|:------------------:|:-----------:|:-----------:|:--------------:|:-------------------------------------------------:|:----------------------:|:--------------:|:----------------:|:-----------:|\r\n|  **project.tag**   | Biological  | Biomedical  | Cardiovascular | Chromosome-centric Human Proteome Project (C-HPP) | Human Proteome Project | Metaproteomics | PRIME-XS Project |  Technical  |\r\n|      **aim**       | 0.01132075  | 0.01838235  |   0.14285714   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.08333333    | 0.02941176  |\r\n|    **analysi**     | 0.19245283  | 0.20220588  |   0.42857143   |                    0.16666667                     |       0.50000000       |   0.13333333   |    0.08333333    | 0.20588235  |\r\n|     **biolog**     | 0.026415094 | 0.007352941 |  0.000000000   |                    0.000000000                    |      0.000000000       |  0.000000000   |   0.083333333    | 0.000000000 |\r\n|    **biomark**     | 0.01886792  | 0.02205882  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|     **brain**      | 0.01132075  | 0.01838235  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **combin**     | 0.02264151  | 0.02941176  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|   **comparison**   | 0.003773585 | 0.011029412 |  0.000000000   |                    0.000000000                    |      0.000000000       |  0.000000000   |   0.166666667    | 0.029411765 |\r\n|   **comprehens**   | 0.03018868  | 0.03308824  |   0.14285714   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|    **control**     | 0.01886792  | 0.01838235  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|    **crucial**     | 0.01132075  | 0.01838235  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|    **dataset**     | 0.02264151  | 0.02573529  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.13333333   |    0.00000000    | 0.02941176  |\r\n|    **determin**    | 0.018867925 | 0.007352941 |  0.000000000   |                    0.000000000                    |      0.000000000       |  0.066666667   |   0.000000000    | 0.029411765 |\r\n|    **develop**     | 0.05283019  | 0.03308824  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|     **differ**     | 0.03018868  | 0.04044118  |   0.14285714   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.11764706  |\r\n|     **digest**     | 0.011320755 | 0.007352941 |  0.142857143   |                    0.000000000                    |      0.000000000       |  0.000000000   |   0.250000000    | 0.058823529 |\r\n|     **enzym**      | 0.01509434  | 0.01838235  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **fluid**      | 0.02641509  | 0.02573529  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **human**      | 0.24905660  | 0.18750000  |   0.14285714   |                    0.00000000                     |       0.50000000       |   0.06666667   |    0.08333333    | 0.17647059  |\r\n|    **identifi**    | 0.04528302  | 0.04044118  |   0.14285714   |                    0.25000000                     |       0.00000000       |   0.00000000   |    0.08333333    | 0.02941176  |\r\n|     **impact**     | 0.01509434  | 0.01102941  |   0.00000000   |                    0.08333333                     |       0.00000000       |   0.06666667   |    0.00000000    | 0.00000000  |\r\n|    **investig**    | 0.02641509  | 0.03308824  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.08333333    | 0.05882353  |\r\n|     **itraq**      | 0.011320755 | 0.007352941 |  0.000000000   |                    0.000000000                    |      0.000000000       |  0.000000000   |   0.083333333    | 0.029411765 |\r\n|      **larg**      | 0.01886792  | 0.01102941  |   0.00000000   |                    0.00000000                     |       0.50000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **major**      | 0.011320755 | 0.007352941 |  0.000000000   |                    0.083333333                    |      0.000000000       |  0.000000000   |   0.083333333    | 0.029411765 |\r\n|      **mass**      | 0.08301887  | 0.05147059  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.05882353  |\r\n|     **modif**      | 0.026415094 | 0.007352941 |  0.000000000   |                    0.000000000                    |      0.000000000       |  0.000000000   |   0.000000000    | 0.029411765 |\r\n|    **pathway**     | 0.02264151  | 0.01470588  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **plasma**     | 0.03018868  | 0.04044118  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.05882353  |\r\n|    **present**     | 0.02264151  | 0.02205882  |   0.00000000   |                    0.16666667                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|  **proteogenom**   | 0.01886792  | 0.01102941  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.13333333   |    0.00000000    | 0.00000000  |\r\n|  **spectrometri**  | 0.05283019  | 0.02573529  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|     **stimul**     | 0.01509434  | 0.01470588  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.02941176  |\r\n|    **strategi**    | 0.02641509  | 0.01838235  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|     **system**     | 0.04905660  | 0.04411765  |   0.14285714   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n|      **time**      | 0.00754717  | 0.02205882  |   0.00000000   |                    0.00000000                     |       0.00000000       |   0.00000000   |    0.00000000    | 0.00000000  |\r\n\r\n\r\n## A meta-data and text model  \r\n\r\nSo let's train now a model using our meta-data predictors together with those\r\nselected in the previous process using textual data.  \r\n\r\n\r\n```r\r\nset.seed(123)\r\nrfModelMetaText <- randomForest(\r\n    project.tags ~ . - accession - publication.date - project.title - project.description, \r\n    data = evalTrain)\r\n# Calculate accuracy on the test set\r\nrfPredMetaText <- predict(rfModelMetaText, newdata=evalTest)\r\nt <- table(evalTest$project.tags, rfPredMetaText)\r\nmeta_text_accuracy <- (t[1,1] + t[2,2] + t[3,3] + t[4,4] + t[5,5] + t[6,6] + t[7,7] + t[8,8]) / nrow(evalTest)\r\n```\r\n\r\nWe obtain an accuracy of **0.75**. The improvement is actually\r\nin better predicting the `biological` and metaproteomics classes a bit.  \r\n\r\n\r\n|                         &nbsp;                          |  Biological  |  Biomedical  |  Cardiovascular  |  Chromosome-centric Human Proteome Project (C-HPP)  |  Human Proteome Project  |  Metaproteomics  |  PRIME-XS Project  |  Technical  |\r\n|:-------------------------------------------------------:|:------------:|:------------:|:----------------:|:---------------------------------------------------:|:------------------------:|:----------------:|:------------------:|:-----------:|\r\n|                     **Biological**                      |      40      |      8       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                     **Biomedical**                      |      8       |      41      |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                   **Cardiovascular**                    |      0       |      1       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|  **Chromosome-centric Human Proteome Project (C-HPP)**  |      0       |      0       |        0         |                          1                          |            0             |        0         |         0          |      0      |\r\n|               **Human Proteome Project**                |      0       |      0       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                   **Metaproteomics**                    |      2       |      3       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                  **PRIME-XS Project**                   |      1       |      2       |        0         |                          0                          |            0             |        0         |         0          |      0      |\r\n|                      **Technical**                      |      2       |      1       |        0         |                          0                          |            0             |        0         |         0          |      2      |\r\n\r\n# Conclusions  \r\n\r\nWe have seen our two main problems:  \r\n\r\n - Our data is very unbalanced. There are many more `biological` and `biomedical`\r\n data than anything else.  \r\n - The previous one becomes more of a problem due to the reduced size of our \r\n training data.  \r\n \r\nHopefully both problems will become less and less important with time, while\r\nPRIDE gets more submissions.  \r\n\r\nWe have also seen that incorporating textual data to meta-data makes our model \r\nmore accurate. Not by a lot, but more accurate after all.  \r\n\r\nIn terms of our original questions, we don't have enough confidence neither\r\nto predict tags nor to profile each tag in terms of its associated meta-data\r\nor textual description (although we have a starting list of candidate terms). \r\nHowever we have seen that there is a relationship, specially when we have \r\nenough cases for a given tag to train a model. Additionally we have seen that \r\nboth things, meta-data and textual description contribute to predict a dataset \r\ntag more accuratelly.  \r\n\r\n# Future works  \r\n\r\nOur simple train/split might be making our model overfit the data. A good approach\r\nwould be to use cross validation. However, by using *random forests* we attenuate\r\nthis problem.  \r\n\r\nWe also need to try different models and do additional exploratory analysis.\r\nBy doing so we will come with additional insight into the nature of our data. This\r\nwill help to select variables and even to use other techniques such as ensemble\r\nor cluster-then-predict.  \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}