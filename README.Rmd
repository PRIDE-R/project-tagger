---
title: "PRIDE Archive Project Tagger"
author: "Jose A. Dianes"
date: "6 April 2015"
output:
  html_document:
    fig_width: 9
    keep_md: yes
    theme: cerulean
---
# Introduction and Goals  

# Getting and cleaning the data  

Let's start by getting the latest `prideR` version form `GitHub`, together with 
some other packages we will use to build our model.  

```{r, message=FALSE}
library(devtools)
install_github("PRIDE-R/prideR")
library(prideR)
require(ggplot2)
theme_set(theme_linedraw())
require(tm)
library(caTools)
library(randomForest)
library(pander)
```

Now we get all the projects available in the archive.  

```{r, cache=TRUE}
archive.df <- as.data.frame(search.list.ProjectSummary("",0,20000))
```

Kepp only those with a project tag assigned.  

```{r}
archive.df <- subset(archive.df, project.tags!="Not available")
```

Now we have `r nrow(archive.df)` tagged projects in the dataset. Let's have a 
quick look at how these tags are distributed.  

```{r}
ggplot(data=archive.df, aes(x=as.factor(archive.df$project.tags))) +
    geom_bar() +
    coord_flip( ) +
    ylab("Number of Projects") +
    xlab("Project Tag")
```

As we see, our data project tag distribution is very skewed. This will make 
difficult to build good models, specially now that we have very few projects.  

# Using meta-data  

Let's start by trying to predict projects tags based on meta-data only. That is,
by using information such as the *species*, *tissues*, and so on.  

## Preparing meta-data  

Right now we have all the metadata associated to PRIDE datasets in text format.
In order to use it to build a model we better convert it to categorica data.
However, most of them have too many differnet values for R machine libraries to
make use of them as predictors. For that reason we need to aggregate those low
frequency values into a unique `Other` factor level.  

```{r}
species.counts <- table(archive.df$species)
low.freq.species <- as.factor(names(which(species.counts<3)))
archive.df[archive.df$species %in% low.freq.species,]$species <- "Other"
archive.df$species <- as.factor(archive.df$species)

tissues.counts <- table(archive.df$tissues)
low.freq.tissues <- as.factor(names(which(tissues.counts<3)))
archive.df[archive.df$tissues %in% low.freq.tissues,]$tissues <- "Other"
archive.df$tissues <- as.factor(archive.df$tissues)

ptm.counts <- table(archive.df$ptm.names)
low.freq.ptm <- as.factor(names(which(ptm.counts<3)))
archive.df[archive.df$ptm.names %in% low.freq.ptm,]$ptm.names <- "Other"
archive.df$ptm.names <- as.factor(archive.df$ptm.names)

instrument.counts <- table(archive.df$instrument.names)
low.freq.instrument <- as.factor(names(which(instrument.counts<3)))
archive.df[archive.df$instrument.names %in% low.freq.instrument,]$instrument.names <- "Other"
archive.df$instrument.names <- as.factor(archive.df$instrument.names)

archive.df$project.tags <- as.factor(archive.df$project.tags)
archive.df$submissionType <- as.factor(archive.df$submissionType)
```

## A meta-data only model  

Let's try now a *Random Forest* model using just metadata predictors. We know 
our dependent variable is very skewed, so don't expect too much accuracy. We 
decide for this type of model due to its success with non-linear problems.  

First of all, prepare a train/test split so we can check accuracy. We also 
have the problem of having too few data in general. Hopefully our models will
get better while PRIDE datasets get more numerous.  

```{r}
set.seed(123)
spl <- sample.split(archive.df, .85)

evalTrain <- archive.df[spl==T,]
evalTest <- archive.df[spl==F,]
```

And now train the model.  

```{r}
set.seed(123)
rfModelMeta <- randomForest(
    project.tags ~ species + tissues + ptm.names + instrument.names + submissionType, 
    data = evalTrain)
# Calculate accuracy on the test set
rfPredMeta <- predict(rfModelMeta, newdata=evalTest)
t <- table(evalTest$project.tags, rfPredMeta)
meta_accuracy <- (t[1,1] + t[2,2] + t[3,3] + t[4,4] + t[5,5] + t[6,6] + t[7,7] + t[8,8]) / nrow(evalTest)
pander(t, split.table = Inf)
```

We have a prediction accuracy of **`r meta_accuracy`** on the test data. We can 
also see how the model struggles to predict on classes with very few cases and
does better with large classes.  


# Using text fields  

Surely PRIDE curators use textual description about datasets in order to assign
a tag to them. Let's try to incorporate that information into our models in order
to predict a dataset tag.  

## Preparing the corpus  

We have two textual fields, `project.title` and  `project.description`. Let's 
prepare a corpus with both of them. In both cases we will reduce them to 
lowercase, remove punctuation and stopwords, and apply stemming.   

```{r, message=FALSE}
library(tm)
evalTrain$AllText <- do.call(paste, evalTrain[,c("project.title","project.description")])
evalTest$AllText <- do.call(paste, evalTest[,c("project.title","project.description")])

corpusAll <- Corpus(VectorSource(c(evalTrain$AllText, evalTest$AllText)))
corpusAll <- tm_map(corpusAll, tolower)
corpusAll <- tm_map(corpusAll, PlainTextDocument)
corpusAll <- tm_map(corpusAll, removePunctuation)
corpusAll <- tm_map(corpusAll, removeWords, stopwords("english"))
corpusAll <- tm_map(corpusAll, stripWhitespace)
corpusAll <- tm_map(corpusAll, stemDocument)
```

We will also keep just those terms appearing in at least 3 percent of the projects.  

```{r, message=FALSE, warning=FALSE}
# Generate term matrix
dtmAll <- DocumentTermMatrix(corpusAll)
sparseAll <- removeSparseTerms(dtmAll, 0.97)
allWords <- data.frame(as.matrix(sparseAll))

colnames(allWords) <- make.names(colnames(allWords))
```

## Selecting significative terms  

We have ended up with **`r ncol(allWords)` possible predictors**. But we can do 
better than this. We are going to train a *linear model* using them and our
dependent variable as outcome. Then we will get those variables that are
statistically significative and incorporate them to the main dataset that
we will use with our final model.  

```{r, message=FALSE, warning=FALSE}
# Find most significative terms
allWordsTrain2 <- head(allWords, nrow(evalTrain))
allWordsTrain2$Popular <- evalTrain$project.tag
logModelAllWords <- glm(Popular~., data=allWordsTrain2, family=binomial)
all_three_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.001))
all_two_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.01))
all_one_star_terms <- names(which(summary(logModelAllWords)$coefficients[,4]<0.05))

# Leave just those terms that are different between popular and unpopular articles
allWords <- subset(allWords, 
                   select=names(allWords) %in% all_one_star_terms)

# Split again
allWordsTrain <- head(allWords, nrow(evalTrain))
allWordsTest <- tail(allWords, nrow(evalTest))

# Add to dataframes
evalTrain <- cbind(evalTrain, allWordsTrain)
evalTest <- cbind(evalTest, allWordsTest)

# Remove all text variable since we don't need it
evalTrain$AllText <- NULL
evalTest$AllText <- NULL
```

We ended up with just **`r ncol(allWords)` predictors** that will be incorporated into our model.  

## A meta-data and text model  

So let's train now a model using our meta-data predictors together with those
selected in the previous process using textual data.  

```{r}
set.seed(123)
rfModelMetaText <- randomForest(
    project.tags ~ . - accession - publication.date - project.title - project.description, 
    data = evalTrain)
# Calculate accuracy on the test set
rfPredMetaText <- predict(rfModelMetaText, newdata=evalTest)
t <- table(evalTest$project.tags, rfPredMetaText)
meta_text_accuracy <- (t[1,1] + t[2,2] + t[3,3] + t[4,4] + t[5,5] + t[6,6] + t[7,7] + t[8,8]) / nrow(evalTest)
pander(t, split.table = Inf)
```

We obtain an accuracy of **`r meta_text_accuracy`**. The improvement is actually
in better predicting the `biological` and metaproteomics classes a bit.  

# Conclusions  

We have seen our two main problems:  

 - Our data is very unbalanced. There are many more `biological` and `biomedical`
 data than anything else.  
 - The previous one becomes more of a problem due to the reduced size of our 
 training data.  
 
Hopefully both problems will become less and less important with time, while
PRIDE gets more submissions.  

We have also seen that incorporating textual data to meta-data makes our model 
more accurate. Not by a lot, but more accurate after all.  

# Future works  

Our simple train/split might be making our model overfit the data. A good approach
would be to use cross validation. However, by using *random forests* we attenuate
this problem.  

We also need to try different models and do additional exploratory analysis.
By doing so we will come with additional insight into the nature of our data. This
will help to select variables and even to use other techniques such as ensemble
or cluster-then-predict.  

